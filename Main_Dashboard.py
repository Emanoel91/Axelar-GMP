import streamlit as st
import pandas as pd
import snowflake.connector
import plotly.graph_objects as go
import plotly.express as px
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.backends import default_backend

# --- Page Config ------------------------------------------------------------------------------------------------------
st.set_page_config(
    page_title="Axelar GMP!",
    page_icon="https://img.cryptorank.io/coins/axelar1663924228506.png",
    layout="wide"
)

# --- Title with Logo -----------------------------------------------------------------------------------------------------
st.markdown(
    """
    <div style="display: flex; align-items: center; gap: 15px;">
        <img src="https://img.cryptorank.io/coins/axelar1663924228506.png" alt="axelar Logo" style="width:60px; height:60px;">
        <h1 style="margin: 0;">Axelar GMP!</h1>
    </div>
    """,
    unsafe_allow_html=True
)

# --- Builder Info ---------------------------------------------------------------------------------------------------------
st.markdown(
    """
    <div style="margin-top: 20px; margin-bottom: 20px; font-size: 16px;">
        <div style="display: flex; align-items: center; gap: 10px;">
            <img src="https://pbs.twimg.com/profile_images/1841479747332608000/bindDGZQ_400x400.jpg" style="width:25px; height:25px; border-radius: 50%;">
            <span>Built by: <a href="https://x.com/0xeman_raz" target="_blank">Eman Raz</a></span>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

st.info("üìäCharts initially display data for a default time range. Select a custom range to view results for your desired period.")
st.info("‚è≥On-chain data retrieval may take a few moments. Please wait while the results load.")

# --- Snowflake Connection ----------------------------------------------------------------------------------------
snowflake_secrets = st.secrets["snowflake"]
user = snowflake_secrets["user"]
account = snowflake_secrets["account"]
private_key_str = snowflake_secrets["private_key"]
warehouse = snowflake_secrets.get("warehouse", "")
database = snowflake_secrets.get("database", "")
schema = snowflake_secrets.get("schema", "")

private_key_pem = f"-----BEGIN PRIVATE KEY-----\n{private_key_str}\n-----END PRIVATE KEY-----".encode("utf-8")
private_key = serialization.load_pem_private_key(
    private_key_pem,
    password=None,
    backend=default_backend()
)
private_key_bytes = private_key.private_bytes(
    encoding=serialization.Encoding.DER,
    format=serialization.PrivateFormat.PKCS8,
    encryption_algorithm=serialization.NoEncryption()
)

conn = snowflake.connector.connect(
    user=user,
    account=account,
    private_key=private_key_bytes,
    warehouse=warehouse,
    database=database,
    schema=schema
)

# --- Date Inputs -------------------------------------------------------
col1, col2, col3 = st.columns(3)

with col1:
    timeframe = st.selectbox("Select Time Frame", ["week", "month", "day"])

with col2:
    start_date = st.date_input("Start Date", value=pd.to_datetime("2023-01-01"))

with col3:
    end_date = st.date_input("End Date", value=pd.to_datetime("2025-08-31"))

st.markdown(
    """
    <div style="background-color:#ff2776; padding:1px; border-radius:10px;">
        <h2 style="color:#000000; text-align:center;">Overview of GMP</h2>
    </div>
    """,
    unsafe_allow_html=True
)
# --- Row 1 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_kpi_data(start_date, end_date):
    
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH overview as (
    WITH axelar_gmp AS (
  
    SELECT  
    created_at,
    LOWER(data:call.chain::STRING) AS source_chain,
    LOWER(data:call.returnValues.destinationChain::STRING) AS destination_chain,
    data:call.transaction.from::STRING AS user,

    CASE 
      WHEN IS_ARRAY(data:amount) OR IS_OBJECT(data:amount) THEN NULL
      WHEN TRY_TO_DOUBLE(data:amount::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:amount::STRING)
      ELSE NULL
    END AS amount,

    CASE 
      WHEN IS_ARRAY(data:value) OR IS_OBJECT(data:value) THEN NULL
      WHEN TRY_TO_DOUBLE(data:value::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:value::STRING)
      ELSE NULL
    END AS amount_usd,

    COALESCE(
      CASE 
        WHEN IS_ARRAY(data:gas:gas_used_amount) OR IS_OBJECT(data:gas:gas_used_amount) 
          OR IS_ARRAY(data:gas_price_rate:source_token.token_price.usd) OR IS_OBJECT(data:gas_price_rate:source_token.token_price.usd) 
        THEN NULL
        WHEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) IS NOT NULL 
          AND TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING) IS NOT NULL 
        THEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) * TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING)
        ELSE NULL
      END,
      CASE 
        WHEN IS_ARRAY(data:fees:express_fee_usd) OR IS_OBJECT(data:fees:express_fee_usd) THEN NULL
        WHEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING)
        ELSE NULL
      END
    ) AS fee,

    id, 
    'GMP' AS "Service", 
    data:symbol::STRING AS raw_asset

  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT created_at, id, user, source_chain, destination_chain,
     "Service", amount, amount_usd, fee

FROM axelar_gmp)

select count(distinct id) as "Total Transactions",
count(distinct user) as "Unique Users",
round(sum(amount_usd)) as "Total Volume",
round(sum(amount_usd)/count(distinct user)) as "Average Volume per User"
from overview
where created_at::date>='{start_str}' and created_at::date<='{end_str}'
    """

    df = pd.read_sql(query, conn)
    return df

# --- Load Data ----------------------------------------------------------------------------------------------------
df_kpi = load_kpi_data(start_date, end_date)

# --- KPI Row ------------------------------------------------------------------------------------------------------
col1, col2, col3, col4 = st.columns(4)

col1.metric(
    label="Total Transactions",
    value=f"üîó{df_kpi["Total Transactions"][0]:,} Txns"
)

col2.metric(
    label="Unique Users",
    value=f"üíº{df_kpi["Unique Users"][0]:,} Wallets"
)

col3.metric(
    label="Total Volume",
    value=f"üí≤{df_kpi["Total Volume"][0]:,}"
)

col4.metric(
    label="Average Volume per User",
    value=f"üí≤{df_kpi["Average Volume per User"][0]:,}"
)

# --- Row 2 ------------------------------------------------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_time_series_data(timeframe, start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH overview as (
WITH axelar_gmp AS (
  
  SELECT  
    created_at,
    LOWER(data:call.chain::STRING) AS source_chain,
    LOWER(data:call.returnValues.destinationChain::STRING) AS destination_chain,
    data:call.transaction.from::STRING AS user,

    CASE 
      WHEN IS_ARRAY(data:amount) OR IS_OBJECT(data:amount) THEN NULL
      WHEN TRY_TO_DOUBLE(data:amount::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:amount::STRING)
      ELSE NULL
    END AS amount,

    CASE 
      WHEN IS_ARRAY(data:value) OR IS_OBJECT(data:value) THEN NULL
      WHEN TRY_TO_DOUBLE(data:value::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:value::STRING)
      ELSE NULL
    END AS amount_usd,

    COALESCE(
      CASE 
        WHEN IS_ARRAY(data:gas:gas_used_amount) OR IS_OBJECT(data:gas:gas_used_amount) 
          OR IS_ARRAY(data:gas_price_rate:source_token.token_price.usd) OR IS_OBJECT(data:gas_price_rate:source_token.token_price.usd) 
        THEN NULL
        WHEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) IS NOT NULL 
          AND TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING) IS NOT NULL 
        THEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) * TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING)
        ELSE NULL
      END,
      CASE 
        WHEN IS_ARRAY(data:fees:express_fee_usd) OR IS_OBJECT(data:fees:express_fee_usd) THEN NULL
        WHEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING)
        ELSE NULL
      END
    ) AS fee,

    id, 
    'GMP' AS "Service", 
    data:symbol::STRING AS raw_asset

  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT created_at, id, user, source_chain, destination_chain,
     "Service", amount, amount_usd, fee

FROM axelar_gmp)

select 
date_trunc('{timeframe}',created_at) as "Date",
count(distinct id) as "Total Transactions",
count(distinct user) as "Unique Users",
round(sum(amount_usd)) as "Total Volume"
from overview
where created_at::date>='{start_str}' and created_at::date<='{end_str}'
group by 1
order by 1
    """

    return pd.read_sql(query, conn)

# --- Load Data ----------------------------------------------------------------------------------------------------
df_ts = load_time_series_data(timeframe, start_date, end_date)
# --- Row 2 charts -------------------------------------------------------------------------------------------------
col1, col2 = st.columns(2)

with col1:
    fig1 = go.Figure()

    fig1.add_bar(
        x=df_ts["Date"], 
        y=df_ts["Total Transactions"], 
        name="Total Transactions", 
        yaxis="y1",
        marker_color="blue"
    )

    fig1.add_trace(go.Scatter(
        x=df_ts["Date"], 
        y=df_ts["Unique Users"], 
        name="Unique Users", 
        mode="lines", 
        yaxis="y2",
        line=dict(color="red")
    ))
    fig1.update_layout(
        title="Number of Users and Transactions Over Time",
        yaxis=dict(title="Txns count"),
        yaxis2=dict(title="Wallet count", overlaying="y", side="right"),
        xaxis=dict(title=" "),
        barmode="group",
        legend=dict(
            orientation="h",   
            yanchor="bottom", 
            y=1.05,           
            xanchor="center",  
            x=0.5
        )
    )
    st.plotly_chart(fig1, use_container_width=True)

with col2:
    fig2 = px.area(df_ts, x="Date", y="Total Volume", title="Volume Over Time ($USD)")
    fig2.update_layout(
        xaxis_title=" ",
        yaxis_title="$USD",
        template="plotly_white"
    )
    st.plotly_chart(fig2, use_container_width=True)

# --- Row 3 ----------------------------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_quarterly_data(timeframe, start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")
    
    query = f"""
  WITH overview as (
  WITH axelar_gmp AS (
  
  SELECT  
    created_at,
    LOWER(data:call.chain::STRING) AS source_chain,
    LOWER(data:call.returnValues.destinationChain::STRING) AS destination_chain,
    data:call.transaction.from::STRING AS user,

    CASE 
      WHEN IS_ARRAY(data:amount) OR IS_OBJECT(data:amount) THEN NULL
      WHEN TRY_TO_DOUBLE(data:amount::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:amount::STRING)
      ELSE NULL
    END AS amount,

    CASE 
      WHEN IS_ARRAY(data:value) OR IS_OBJECT(data:value) THEN NULL
      WHEN TRY_TO_DOUBLE(data:value::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:value::STRING)
      ELSE NULL
    END AS amount_usd,

    COALESCE(
      CASE 
        WHEN IS_ARRAY(data:gas:gas_used_amount) OR IS_OBJECT(data:gas:gas_used_amount) 
          OR IS_ARRAY(data:gas_price_rate:source_token.token_price.usd) OR IS_OBJECT(data:gas_price_rate:source_token.token_price.usd) 
        THEN NULL
        WHEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) IS NOT NULL 
          AND TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING) IS NOT NULL 
        THEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) * TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING)
        ELSE NULL
      END,
      CASE 
        WHEN IS_ARRAY(data:fees:express_fee_usd) OR IS_OBJECT(data:fees:express_fee_usd) THEN NULL
        WHEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING)
        ELSE NULL
      END
    ) AS fee,

    id, 
    'GMP' AS "Service", 
    data:symbol::STRING AS raw_asset

  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT created_at, id, user, source_chain, destination_chain,
     "Service", amount, amount_usd, fee

FROM axelar_gmp)

select 
date_trunc('{timeframe}',created_at) as "Date", case 
when created_at::date >= '2022-01-01' and created_at::date < '2022-03-31' then 'Q1-2022'
when created_at::date >= '2022-04-01' and created_at::date < '2022-06-30' then 'Q2-2022'
when created_at::date >= '2022-07-01' and created_at::date < '2022-09-30' then 'Q3-2022'
when created_at::date >= '2022-10-01' and created_at::date < '2022-12-31' then 'Q4-2022'
when created_at::date >= '2023-01-01' and created_at::date < '2023-03-31' then 'Q1-2023'
when created_at::date >= '2023-04-01' and created_at::date < '2023-06-30' then 'Q2-2023'
when created_at::date >= '2023-07-01' and created_at::date < '2023-09-30' then 'Q3-2023'
when created_at::date >= '2023-10-01' and created_at::date < '2023-12-31' then 'Q4-2023'
when created_at::date >= '2024-01-01' and created_at::date < '2024-03-31' then 'Q1-2024'
when created_at::date >= '2024-04-01' and created_at::date < '2024-06-30' then 'Q2-2024'
when created_at::date >= '2024-07-01' and created_at::date < '2024-09-30' then 'Q3-2024'
when created_at::date >= '2024-10-01' and created_at::date < '2024-12-31' then 'Q4-2024' 
when created_at::date >= '2025-01-01' and created_at::date < '2025-03-31' then 'Q1-2025'
when created_at::date >= '2025-04-01' and created_at::date < '2025-06-30' then 'Q2-2025'
when created_at::date >= '2025-07-01' and created_at::date < '2025-09-30' then 'Q3-2025'
when created_at::date >= '2025-10-01' and created_at::date < '2025-12-31' then 'Q4-2025'
when created_at::date >= '2026-01-01' and created_at::date < '2026-03-31' then 'Q1-2026'
when created_at::date >= '2026-04-01' and created_at::date < '2026-06-30' then 'Q2-2026'
when created_at::date >= '2026-07-01' and created_at::date < '2026-09-30' then 'Q3-2026'
when created_at::date >= '2026-10-01' and created_at::date < '2026-12-31' then 'Q4-2026'
end as "Quarter",
round(sum(amount_usd)) as "Total Volume",
sum("Total Volume") over (partition by "Quarter" order by "Date" asc) as "Cumulative Volume"
from overview
where created_at::date>='{start_str}' and created_at::date<='{end_str}'
group by 1, 2
order by 1

    """
    return pd.read_sql(query, conn)
# --- Load Data --------------------------------------------------------------
quarterly_data = load_quarterly_data(timeframe, start_date, end_date)
# --- stacked bar Chart ------------------------------------------------------
fig_stacked = px.bar(
    quarterly_data,
    x="Date",
    y="Cumulative Volume",
    color="Quarter",
    title="Volume per Quarter Over Time (USD)"
)
fig_stacked.update_layout(barmode="stack", yaxis_title="$USD")
st.plotly_chart(fig_stacked, use_container_width=True)

# --- Row 4 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_kpi_data_chains(start_date, end_date):
    
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH overview as (
    WITH axelar_gmp AS (
  
    SELECT  
    created_at,
    LOWER(data:call.chain::STRING) AS source_chain,
    LOWER(data:call.returnValues.destinationChain::STRING) AS destination_chain,
    data:call.transaction.from::STRING AS user,

    CASE 
      WHEN IS_ARRAY(data:amount) OR IS_OBJECT(data:amount) THEN NULL
      WHEN TRY_TO_DOUBLE(data:amount::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:amount::STRING)
      ELSE NULL
    END AS amount,

    CASE 
      WHEN IS_ARRAY(data:value) OR IS_OBJECT(data:value) THEN NULL
      WHEN TRY_TO_DOUBLE(data:value::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:value::STRING)
      ELSE NULL
    END AS amount_usd,

    COALESCE(
      CASE 
        WHEN IS_ARRAY(data:gas:gas_used_amount) OR IS_OBJECT(data:gas:gas_used_amount) 
          OR IS_ARRAY(data:gas_price_rate:source_token.token_price.usd) OR IS_OBJECT(data:gas_price_rate:source_token.token_price.usd) 
        THEN NULL
        WHEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) IS NOT NULL 
          AND TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING) IS NOT NULL 
        THEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) * TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING)
        ELSE NULL
      END,
      CASE 
        WHEN IS_ARRAY(data:fees:express_fee_usd) OR IS_OBJECT(data:fees:express_fee_usd) THEN NULL
        WHEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING)
        ELSE NULL
      END
    ) AS fee,

    id, 
    'GMP' AS "Service", 
    data:symbol::STRING AS raw_asset

  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT created_at, id, user, source_chain, destination_chain,
     "Service", amount, amount_usd, fee

FROM axelar_gmp)

select 
count(distinct source_chain) as "Number of Sources",
count(distinct destination_chain) as "Number of Destinations",
round(avg(amount_usd)) as "Average Volume",
round(max(amount_usd)) as "Max Volumme"
from overview
where created_at::date>='{start_str}' and created_at::date<='{end_str}'
    """

    df = pd.read_sql(query, conn)
    return df

# --- Load Data ----------------------------------------------------------------------------------------------------
df_kpi_chains = load_kpi_data_chains(start_date, end_date)

# --- KPI Row ------------------------------------------------------------------------------------------------------
col1, col2, col3, col4 = st.columns(4)

col1.metric(
    label="Number of Sources",
    value=f"‚õì{df_kpi_chains["Number of Sources"][0]:,} Chains"
)

col2.metric(
    label="Number of Destinations",
    value=f"‚õì{df_kpi_chains["Number of Destinations"][0]:,} Chains"
)

col3.metric(
    label="Average Volume",
    value=f"üí≤{df_kpi_chains["Average Volume"][0]:,}"
)

col4.metric(
    label="Max Volumme",
    value=f"üí≤{df_kpi_chains["Max Volumme"][0]:,}"
)

# --- Row 5 -------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_chain_data_over_time(timeframe, start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH overview as (
WITH axelar_gmp AS (
  
  SELECT  
    created_at,
    LOWER(data:call.chain::STRING) AS source_chain,
    LOWER(data:call.returnValues.destinationChain::STRING) AS destination_chain,
    data:call.transaction.from::STRING AS user,

    CASE 
      WHEN IS_ARRAY(data:amount) OR IS_OBJECT(data:amount) THEN NULL
      WHEN TRY_TO_DOUBLE(data:amount::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:amount::STRING)
      ELSE NULL
    END AS amount,

    CASE 
      WHEN IS_ARRAY(data:value) OR IS_OBJECT(data:value) THEN NULL
      WHEN TRY_TO_DOUBLE(data:value::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:value::STRING)
      ELSE NULL
    END AS amount_usd,

    COALESCE(
      CASE 
        WHEN IS_ARRAY(data:gas:gas_used_amount) OR IS_OBJECT(data:gas:gas_used_amount) 
          OR IS_ARRAY(data:gas_price_rate:source_token.token_price.usd) OR IS_OBJECT(data:gas_price_rate:source_token.token_price.usd) 
        THEN NULL
        WHEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) IS NOT NULL 
          AND TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING) IS NOT NULL 
        THEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) * TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING)
        ELSE NULL
      END,
      CASE 
        WHEN IS_ARRAY(data:fees:express_fee_usd) OR IS_OBJECT(data:fees:express_fee_usd) THEN NULL
        WHEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING)
        ELSE NULL
      END
    ) AS fee,

    id, 
    'GMP' AS "Service", 
    data:symbol::STRING AS raw_asset

  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT created_at, id, user, source_chain, destination_chain,
     "Service", amount, amount_usd, fee

FROM axelar_gmp)

select date_trunc('{timeframe}',created_at) as "Date",
count(distinct source_chain) as "Sources",
count(distinct destination_chain) as "Destinations"
from overview
where created_at::date>='{start_str}' and created_at::date<='{end_str}'
group by 1
order by 1

    """

    return pd.read_sql(query, conn)

@st.cache_data
def load_moving_average_data(timeframe, start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH overview as (
WITH axelar_gmp AS (
  
  SELECT  
    created_at,
    LOWER(data:call.chain::STRING) AS source_chain,
    LOWER(data:call.returnValues.destinationChain::STRING) AS destination_chain,
    data:call.transaction.from::STRING AS user,

    CASE 
      WHEN IS_ARRAY(data:amount) OR IS_OBJECT(data:amount) THEN NULL
      WHEN TRY_TO_DOUBLE(data:amount::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:amount::STRING)
      ELSE NULL
    END AS amount,

    CASE 
      WHEN IS_ARRAY(data:value) OR IS_OBJECT(data:value) THEN NULL
      WHEN TRY_TO_DOUBLE(data:value::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:value::STRING)
      ELSE NULL
    END AS amount_usd,

    COALESCE(
      CASE 
        WHEN IS_ARRAY(data:gas:gas_used_amount) OR IS_OBJECT(data:gas:gas_used_amount) 
          OR IS_ARRAY(data:gas_price_rate:source_token.token_price.usd) OR IS_OBJECT(data:gas_price_rate:source_token.token_price.usd) 
        THEN NULL
        WHEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) IS NOT NULL 
          AND TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING) IS NOT NULL 
        THEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) * TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING)
        ELSE NULL
      END,
      CASE 
        WHEN IS_ARRAY(data:fees:express_fee_usd) OR IS_OBJECT(data:fees:express_fee_usd) THEN NULL
        WHEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING)
        ELSE NULL
      END
    ) AS fee,

    id, 
    'GMP' AS "Service", 
    data:symbol::STRING AS raw_asset

  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT created_at, id, user, source_chain, destination_chain,
     "Service", amount, amount_usd, fee

FROM axelar_gmp)

select 
date_trunc('{timeframe}',created_at) as "Date",
round(sum(amount_usd)) as usd_volume,
avg(usd_volume) over (order by "Date" rows between 4 preceding and current row) as "Avg 30 Day Moving",
avg(usd_volume) over (order by "Date" rows between 8 preceding and current row) as "Avg 60 Day Moving",
avg(usd_volume) over (order by "Date" rows between 12 preceding and current row) as "Avg 90 Day Moving"
from overview
where created_at::date>='{start_str}' and created_at::date<='{end_str}'
group by 1
order by 1

    """

    return pd.read_sql(query, conn)
# --- Load Data ----------------------------------------------------------------------------------------------------
chain_data_over_time = load_chain_data_over_time(timeframe, start_date, end_date)
moving_average_data = load_moving_average_data(timeframe, start_date, end_date)
# ------------------------------------------------------------------------------------------------------------------

col1, col2 = st.columns(2)

with col1:
    fig1 = go.Figure()
    fig1.add_trace(
        go.Scatter(
            x=chain_data_over_time["Date"],
            y=chain_data_over_time["Sources"],
            name="Sources",
            mode="lines",
            yaxis="y1"
        )
    )
    
    fig1.add_trace(
        go.Scatter(
            x=chain_data_over_time["Date"],
            y=chain_data_over_time["Destinations"],
            name="Destinations",
            mode="lines",
            yaxis="y1"
        )
    )

    fig1.update_layout(
        title="Number of Active Chains Over Time",
        yaxis=dict(title="Chain count"),
        xaxis=dict(title=" "),
        legend=dict(
            orientation="h",   
            yanchor="bottom", 
            y=1.05,           
            xanchor="center",  
            x=0.5
        )
    )
    st.plotly_chart(fig1, use_container_width=True)

with col2:
    fig2 = go.Figure()
    fig2.add_trace(
        go.Scatter(
            x=moving_average_data["Date"],
            y=moving_average_data["Avg 30 Day Moving"],
            name="Avg 30 Day Moving",
            mode="lines",
            yaxis="y1"
        )
    )
    
    fig2.add_trace(
        go.Scatter(
            x=moving_average_data["Date"],
            y=moving_average_data["Avg 60 Day Moving"],
            name="Avg 60 Day Moving",
            mode="lines",
            yaxis="y1"
        )
    )

    fig2.add_trace(
        go.Scatter(
            x=moving_average_data["Date"],
            y=moving_average_data["Avg 90 Day Moving"],
            name="Avg 90 Day Moving",
            mode="lines",
            yaxis="y1"
        )
    )
    
    fig2.update_layout(
        title="Average 30, 60 & 90 Moving Volume Over Time",
        yaxis=dict(title="$USD"),
        xaxis=dict(title=" "),
        legend=dict(
            orientation="h",   
            yanchor="bottom", 
            y=1.05,           
            xanchor="center",  
            x=0.5
        )
    )
    st.plotly_chart(fig2, use_container_width=True)

st.markdown(
    """
    <div style="background-color:#ff2776; padding:1px; border-radius:10px;">
        <h2 style="color:#000000; text-align:center;">Analysis of Users</h2>
    </div>
    """,
    unsafe_allow_html=True
)
# --- Row 6 --------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_txn_distribution(start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH axelar_gmp AS (
SELECT data:call.transaction.from::STRING AS user, count(distinct id), case 
when count(distinct id)=1 then '1 Txn'
when count(distinct id)=2 then '2 Txns'
when count(distinct id)>=3 and count(distinct id)<=5 then '3-5 Txns'
when count(distinct id)>=6 and count(distinct id)<=10 then '6-10 Txns'
when count(distinct id)>=11 and count(distinct id)<=15 then '11-15 Txns'
when count(distinct id)>=16 and count(distinct id)<=25 then '16-25 Txns'
when count(distinct id)>=26 and count(distinct id)<=50 then '26-50 Txns'
when count(distinct id)>=51  then '>50 Txns'
end as "Class"
FROM axelar.axelscan.fact_gmp 
WHERE status = 'executed' AND simplified_status = 'received'
and created_at::date>='2022-11-01' and created_at::date<='2025-08-31'
group by 1)

select "Class", count(distinct user) as "Number of Users"
From axelar_gmp
GROUP BY 1
ORDER BY 2 desc
    """

    return pd.read_sql(query, conn)

# --- Load Data --------------------------------------------------------------------------------------
txn_distribution = load_txn_distribution(start_date, end_date)
# ----------------------------------------------------------------------------------------------------
bar_fig = px.bar(
    txn_distribution,
    x="Class",
    y="Number of Users",
    title="Breakdown of Users",
    color_discrete_sequence=["#5e67f8"]
)
bar_fig.update_layout(
    xaxis_title=" ",
    yaxis_title="Wallet count",
    bargap=0.2
)

# ---------------------------------------
color_scale = {
    '1 Txn': '#d9fd51',        # lime-ish
    '2 Txns': '#b1f85a',
    '3-5 Txns': '#8be361',
    '6-10 Txns': '#639d55',
    '11-15 Txns': '#4a7c42',
    '16-25 Txns': '#7a4c89',  # purple-ish
    '26-50 Txns': '#cd00fc',
    '>50 Txns': '#fa1d64',
}

fig_donut_volume = px.pie(
    txn_distribution,
    names="Class",
    values="Number of Users",
    title="Share of Users",
    hole=0.5,
    color="Class",
    color_discrete_map=color_scale
)

fig_donut_volume.update_traces(textposition='outside', textinfo='percent+label', pull=[0.05]*len(txn_distribution))
fig_donut_volume.update_layout(showlegend=True, legend=dict(orientation="v", y=0.5, x=1.1))

col1, col2 = st.columns(2)

with col1:
    st.plotly_chart(bar_fig, use_container_width=True)

with col2:
    st.plotly_chart(fig_donut_volume, use_container_width=True)

# --- Row 7 --------------------------------------------------------------------------------------------------------
@st.cache_data
def load_new_users_data(timeframe, start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH overview as (
WITH axelar_gmp AS (
  
  SELECT  
    created_at,
    data:call.transaction.from::STRING AS user
  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT user, min(created_at::date) as first_txn_date
FROM axelar_gmp
group by 1)

select 
date_trunc('{timeframe}',first_txn_date) as "Date",
count(distinct user) as "New Users",
sum("New Users") over (order by "Date") as "Cumulative New Users"
from overview
where first_txn_date::date>='{start_str}' and first_txn_date::date<='{end_str}'
group by 1
order by 1

    """

    return pd.read_sql(query, conn)

# --- Load Data ----------------------------------------------------------------------------------------------------
new_users_data = load_new_users_data(timeframe, start_date, end_date)
# --- Row 3 --------------------------------------------------------------------------------------------------------

fig1 = go.Figure()

fig1.add_trace(go.Bar(
    x=new_users_data["Date"], 
    y=new_users_data["New Users"], 
    name="New Users", 
    yaxis="y1",
    marker_color="blue"
))

fig1.add_trace(go.Scatter(
    x=new_users_data["Date"], 
    y=new_users_data["Cumulative New Users"], 
    name="Cumulative New Users", 
    mode="lines", 
    yaxis="y2",
    line=dict(color="red")
))

fig1.update_layout(
    title="Number of New Users Over Time",
    yaxis=dict(title="Wallet count"),  
    yaxis2=dict(title="Wallet count", overlaying="y", side="right"),  
    xaxis=dict(title=" "),
    barmode="group",
    legend=dict(
        orientation="h",   
        yanchor="bottom", 
        y=1.05,           
        xanchor="center",  
        x=0.5
    )
)
st.plotly_chart(fig1, use_container_width=True)

# --- Row 8 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_kpi_data_new_user(start_date, end_date):
    
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH axelar_gmp AS (
    SELECT  
        created_at::date AS created_date,
        data:call.transaction.from::STRING AS user
    FROM axelar.axelscan.fact_gmp 
    WHERE status = 'executed'
      AND simplified_status = 'received'
),
overview AS (
    SELECT 
        user, 
        MIN(created_date) AS first_txn_date
    FROM axelar_gmp
    GROUP BY user
),
daily_new_users AS (
    SELECT 
        DATE_TRUNC('day', first_txn_date) AS date,
        COUNT(DISTINCT user) AS new_users
    FROM overview
    WHERE first_txn_date BETWEEN '{start_str}' AND '{end_str}'
    GROUP BY 1
),
stats AS (
    SELECT 
        MAX(SUM(new_users)) OVER (ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) 
            AS cumulative_new_users,
        ROUND(AVG(new_users)) AS average_daily_new_users
    FROM daily_new_users
)
SELECT DISTINCT cumulative_new_users, average_daily_new_users
FROM stats

    """

    df = pd.read_sql(query, conn)
    return df

# --- Load Data ----------------------------------------------------------------------------------------------------
kpi_data_new_user = load_kpi_data_new_user(start_date, end_date)

# --- KPI Row ------------------------------------------------------------------------------------------------------
col1, col2 = st.columns(2)

col1.metric(
    label="Total New Users",
    value=f"üë•{kpi_data_new_user["CUMULATIVE_NEW_USERS"][0]:,} Wallets"
)

col2.metric(
    label="Average Daily New Users",
    value=f"üíº{kpi_data_new_user["AVERAGE_DAILY_NEW_USERS"][0]:,} Wallets"
)
# --- Row 9 --------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_pie_data_txn(start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH axelar_gmp AS (
SELECT data:call.transaction.from::STRING AS user, count(distinct id), case 
when count(distinct id)=1 then '1 Txn'
when count(distinct id)=2 then '2 Txns'
when count(distinct id)=3 then '3 Txns'
when count(distinct id)=4 then '4 Txn'
when count(distinct id)=5 then '5 Txns'
when count(distinct id)=6 then '6 Txns'
when count(distinct id)=7 then '7 Txn'
when count(distinct id)=8 then '8 Txns'
when count(distinct id)=9 then '9 Txns'
when count(distinct id)=10 then '10 Txns'
when count(distinct id)>=11  then '>10 Txns'
end as "Number of Txns"
FROM axelar.axelscan.fact_gmp 
WHERE status = 'executed' AND simplified_status = 'received'
and created_at::date>='{start_str}' and created_at::date<='{end_str}'
group by 1)

select "Number of Txns", count(distinct user) as "Number of Users"
From axelar_gmp
GROUP BY 1
ORDER BY 2 desc

    """

    return pd.read_sql(query, conn)

@st.cache_data
def load_pie_data_day(start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH axelar_gmp AS (
SELECT data:call.transaction.from::STRING AS user, count(distinct created_at::date), case 
when count(distinct created_at::date)=1 then '1 Day'
when count(distinct created_at::date)=2 then '2 Days'
when count(distinct created_at::date)=3 then '3 Days'
when count(distinct created_at::date)=4 then '4 Days'
when count(distinct created_at::date)=5 then '5 Days'
when count(distinct created_at::date)=6 then '6 Days'
when count(distinct created_at::date)=7 then '7 Days'
when count(distinct created_at::date)=8 then '8 Days'
when count(distinct created_at::date)=9 then '9 Days'
when count(distinct created_at::date)=10 then '10 Days'
when count(distinct created_at::date)>=11  then '>10 Days'
end as "#Days of Activity"
FROM axelar.axelscan.fact_gmp 
WHERE status = 'executed' AND simplified_status = 'received'
and created_at::date>='{start_str}' and created_at::date<='{end_str}'
group by 1)

select "#Days of Activity", count(distinct user) as "Number of Users"
From axelar_gmp
GROUP BY 1
ORDER BY 2 desc
    """

    return pd.read_sql(query, conn)

@st.cache_data
def load_pie_data_path(start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    with overview as (
WITH axelar_gmp AS (
SELECT data:call.transaction.from::STRING AS user, 
lower(data:call.chain::STRING) AS source_chain,
lower(data:call.returnValues.destinationChain::STRING) AS destination_chain
FROM axelar.axelscan.fact_gmp 
WHERE status = 'executed' AND simplified_status = 'received'
and created_at::date>='{start_str}' and created_at::date<='{end_str}')

select user, count(distinct (source_chain || '‚û°' || destination_chain)), case 
when count(distinct (source_chain || '‚û°' || destination_chain))=1 then '1 Path'
when count(distinct (source_chain || '‚û°' || destination_chain))=2 then '2 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=3 then '3 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=4 then '4 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=5 then '5 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=6 then '6 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=7 then '7 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=8 then '8 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=9 then '9 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))=10 then '10 Paths'
when count(distinct (source_chain || '‚û°' || destination_chain))>=11  then '>10 Paths'
end as "Number of Paths"
from axelar_gmp 
group by 1)

select "Number of Paths", count(distinct user) as "Number of Users"
from overview
group by 1
order by 2 desc 
    """

    return pd.read_sql(query, conn)

# --- Load Data ----------------------------------------------------------------------------------------------------
pie_data_txn = load_pie_data_txn(start_date, end_date)
pie_data_day = load_pie_data_day(start_date, end_date)
pie_data_path = load_pie_data_path(start_date, end_date)
# --- Layout -------------------------------------------------------------------------------------------------------
col1, col2, col3 = st.columns(3)

# Pie Chart for Txn Distribution
fig1 = px.pie(
    pie_data_txn, 
    values="Number of Users",    
    names="Number of Txns",    
    title="Share of Transaction by Users"
)
fig1.update_traces(textinfo="percent+label", textposition="inside", automargin=True)

# Pie Chart for #Days of Activity
fig2 = px.pie(
    pie_data_day, 
    values="Number of Users",     
    names="#Days of Activity",    
    title="Share of Active Day by Users"
)
fig2.update_traces(textinfo="percent+label", textposition="inside", automargin=True)

# Pie Chart for path
fig3 = px.pie(
    pie_data_path, 
    values="Number of Users",     
    names="Number of Paths",    
    title="Share of Paths by Users"
)
fig3.update_traces(textinfo="percent+label", textposition="inside", automargin=True)

# display charts
col1.plotly_chart(fig1, use_container_width=True)
col2.plotly_chart(fig2, use_container_width=True)
col3.plotly_chart(fig3, use_container_width=True)

st.markdown(
    """
    <div style="background-color:#ff2776; padding:1px; border-radius:10px;">
        <h2 style="color:#000000; text-align:center;">Heatmap</h2>
    </div>
    """,
    unsafe_allow_html=True
)
# --- Row 10 ------------------------------------------------------------------------------------------------------------------------------------------------------
@st.cache_data
def load_heatmap_data(timeframe, start_date, end_date):
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    query = f"""
    WITH overview as (
WITH axelar_gmp AS (
  
  SELECT  
    created_at,
    LOWER(data:call.chain::STRING) AS source_chain,
    LOWER(data:call.returnValues.destinationChain::STRING) AS destination_chain,
    data:call.transaction.from::STRING AS user,

    CASE 
      WHEN IS_ARRAY(data:amount) OR IS_OBJECT(data:amount) THEN NULL
      WHEN TRY_TO_DOUBLE(data:amount::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:amount::STRING)
      ELSE NULL
    END AS amount,

    CASE 
      WHEN IS_ARRAY(data:value) OR IS_OBJECT(data:value) THEN NULL
      WHEN TRY_TO_DOUBLE(data:value::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:value::STRING)
      ELSE NULL
    END AS amount_usd,

    COALESCE(
      CASE 
        WHEN IS_ARRAY(data:gas:gas_used_amount) OR IS_OBJECT(data:gas:gas_used_amount) 
          OR IS_ARRAY(data:gas_price_rate:source_token.token_price.usd) OR IS_OBJECT(data:gas_price_rate:source_token.token_price.usd) 
        THEN NULL
        WHEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) IS NOT NULL 
          AND TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING) IS NOT NULL 
        THEN TRY_TO_DOUBLE(data:gas:gas_used_amount::STRING) * TRY_TO_DOUBLE(data:gas_price_rate:source_token.token_price.usd::STRING)
        ELSE NULL
      END,
      CASE 
        WHEN IS_ARRAY(data:fees:express_fee_usd) OR IS_OBJECT(data:fees:express_fee_usd) THEN NULL
        WHEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING) IS NOT NULL THEN TRY_TO_DOUBLE(data:fees:express_fee_usd::STRING)
        ELSE NULL
      END
    ) AS fee,

    id, 
    'GMP' AS "Service", 
    data:symbol::STRING AS raw_asset

  FROM axelar.axelscan.fact_gmp 
  WHERE status = 'executed'
    AND simplified_status = 'received'
    )

SELECT created_at, id, user, source_chain, destination_chain,
     "Service", amount, amount_usd, fee

FROM axelar_gmp)

select 
DATE_PART(hour, created_at) AS "Hour",
  case
    WHEN EXTRACT(DOW FROM created_at) = 0 THEN 'Sunday'
    WHEN EXTRACT(DOW FROM created_at) = 1 THEN 'Monday'
    WHEN EXTRACT(DOW FROM created_at) = 2 THEN 'Tuesday'
    WHEN EXTRACT(DOW FROM created_at) = 3 THEN 'Wednesday'
    WHEN EXTRACT(DOW FROM created_at) = 4 THEN 'Thursday'
    WHEN EXTRACT(DOW FROM created_at) = 5 THEN 'Friday'
    WHEN EXTRACT(DOW FROM created_at) = 6 THEN 'Saturday'
  END AS "Day",
count(distinct id) as "Number of Transfers",
round(sum(amount_usd)) as "Volume of Transfers"
from overview
where created_at::date>='{start_str}' and created_at::date<='{end_str}'
group by 1, 2
order by 1, 2
    """

    return pd.read_sql(query, conn)

# --- Load Data ----------------------------------------------------------------------------------------------------
heatmap_data = load_heatmap_data(timeframe, start_date, end_date)
# --- Row 2 charts -------------------------------------------------------------------------------------------------
heatmap_data["Hour"] = heatmap_data["Hour"].astype(str)
day_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
heatmap_data["Day"] = pd.Categorical(heatmap_data["Day"], categories=day_order, ordered=True)
col1, col2 = st.columns(2)

with col1:
    st.subheader("Number of Transfers")
    fig1 = px.density_heatmap(
        heatmap_data,
        x="Hour",
        y="Day",
        z="Number of Transfers",
        category_orders={"Day": day_order, "Hour": [str(h) for h in range(24)]},
        color_continuous_scale="Viridis"
    )
    st.plotly_chart(fig1, use_container_width=True)

with col2:
    st.subheader("Volume of Transfers (USD)")
    fig2 = px.density_heatmap(
        heatmap_data,
        x="Hour",
        y="Day",
        z="Volume of Transfers",
        category_orders={"Day": day_order, "Hour": [str(h) for h in range(24)]},
        color_continuous_scale="Plasma"
    )
    st.plotly_chart(fig2, use_container_width=True)
